<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mathias Unberath on Mathias Unberath</title>
    <link>https://mathiasunberath.github.io/</link>
    <description>Recent content in Mathias Unberath on Mathias Unberath</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>3 Papers at MICCAI 2019</title>
      <link>https://mathiasunberath.github.io/talk/miccai_papers_19/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_papers_19/</guid>
      <description>&lt;p&gt;We will be presenting 3 papers at MICCAI 2019!&lt;/p&gt;

&lt;p&gt;J.-N. Zaech, C. Gao, B. Bier, R. Taylor, A. Maier, N. Navab, M. Unberath.
“Learning to Avoid Poor Images: Towards Task-aware C-arm Cone-beam
CT Trajectories”
Received a MICCAI Graduate Student Travel Award based on its review scores.
Selected for &lt;strong&gt;oral&lt;/strong&gt; presentation.&lt;/p&gt;

&lt;p&gt;L. Fink, S.C. Lee, J.Y. Wu, X. Liu, T. Song, Y. Stoyanova, M. Stamminger,
N. Navab, M. Unberath. &amp;ldquo;LumiPath - Towards Real-time Physicallybased
Rendering on Embedded Devices&amp;rdquo; &lt;a href=&#34;https://github.com/lorafib/LumiPath&#34; target=&#34;_blank&#34;&gt;Link to GitHub&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1903.03837&#34; target=&#34;_blank&#34;&gt;Link to preprint&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;J. Esteban, M. Grimm, M. Unberath, G. Zahnd, N. Navab. &amp;ldquo;Towards fully automatic X-ray to CT registration&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 Papers at MICCAI 2018</title>
      <link>https://mathiasunberath.github.io/talk/miccai_papers/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_papers/</guid>
      <description>&lt;p&gt;All our 5 manuscripts submitted to MICCAI 2018 were accepted for presentation (3 early accepts)!&lt;/p&gt;

&lt;p&gt;Pre-prints of our papers are available from arXiv and videos are available from &lt;a href=&#34;https://camp.lcsr.jhu.edu/miccai-2018-demonstration-videos/&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt; (&lt;sup&gt;&amp;#42;&lt;/sup&gt; starred authors are considered joint first):&lt;/p&gt;

&lt;p&gt;M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech&lt;sup&gt;&amp;#42;&lt;/sup&gt;, S. C. Lee, B. Bier, J. Fotouhi, M. Armand,
N. Navab. “DeepDRR: A Catalyst for Machine Learning in Fluoroscopy-guided Procedures”.&lt;/p&gt;

&lt;p&gt;J. Fotouhi&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, G. Taylor&lt;sup&gt;&amp;#42;&lt;/sup&gt;, A. G. Farashahi, B. Bier, R. Taylor,
G. Osgood, M. Armand, N. Navab. “Exploiting Partial Structural Symmetry for Patient-Specific Image Augmentation in Trauma Interventions.&lt;/p&gt;

&lt;p&gt;B. Bier&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech, J. Fotouhi, M. Armand, G. Osgood,
N. Navab, A. Maier. “X-ray-transform Invariant Anatomical Landmark Detection for Pelvic Trauma Surgery”.&lt;/p&gt;

&lt;p&gt;A. Preuhs, A. Maier, M. Manhart, J. Fotouhi, N. Navab, M. Unberath.
“Double Your Views – Expoiting Symmetry in Transmission Imaging”.&lt;/p&gt;

&lt;p&gt;J. Hajek&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J. Fotouhi&lt;sup&gt;&amp;#42;&lt;/sup&gt;, B. Bier, S. C. Lee, G. Osgood, A. Maier, M. Armand, N. Navab. “Closing the Calibration Loop: An Inside-out Tracking Paradigm for Augmented Reality in Orthopedic Surgery”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Another DELTA Grant Awarded</title>
      <link>https://mathiasunberath.github.io/talk/delta_awards_2019/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/delta_awards_2019/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Significance:&lt;/strong&gt; Machine learning and artificial intelligence (ML &amp;amp; AI) techniques are rapidly becoming a staple in healthcare research, but both healthcare and engineering workforces at Johns Hopkins lag behind in their capacity to advance innovation through interdisciplinary collaboration and to assimilate it into practice. Specifically, healthcare scientists need to understand machine learning algorithms, and engineers need to learn biases in healthcare data that affect performance and utility of algorithms. Addressing these learning needs is critical to foster interdisciplinary collaboration at Johns Hopkins and to enable the School of Medicine to shape the future of healthcare innovations through engineering (Goals 2 and 3 in JHU’s 10 by 20 priorities). Our goal is to develop an online course to equip clinicians and engineers with critical thinking skills to design, analyze, interpret, and report research on ML &amp;amp; AI in healthcare. The primary target population for this course, for purposes of this grant, is clinical care providers, faculties, and graduate trainees (residents, clinical &amp;amp; research post-doctoral fellows, doctoral students, and research staff) in the School of Medicine and the Whiting School of Engineering at Johns Hopkins. While we target a specific population for this grant, we have secured preliminary approval from the Engineering for Professionals Program at JHU for MOOC development of our course, which can enhance global visibility and impact of JHU as a leader in driving healthcare innovation using technology.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Innovation and Approach:&lt;/strong&gt; To our knowledge, none of the courses or curricula at Johns Hopkins address the comprehensive learning needs to build capacity in ML &amp;amp; AI in healthcare. The proposed course integrates principles from engineering and statistical sciences with two complementary parts. The first part is focused upon essential concepts of state-of-the-art machine learning methods. The second part is focused upon core concepts of design, bias, evaluation, and reporting in studies on engineering in healthcare. This course brings together educators with engineering, statistical, epidemiological, and clinical experience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evaluation and Expected Outcome:&lt;/strong&gt; In individual learners, we will evaluate skill acquisition using pre- and post-tests focused on the learning objectives of the course. To quantify impact on the University’s 10 by 20 priorities, we will survey Johns Hopkins personnel completing the course to enumerate downstream effects such as incident journal club sessions they lead within their divisions, new research studies, and collaborations focused upon ML &amp;amp; AI in healthcare.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emerson Collective Cancer Research Fund Awarded</title>
      <link>https://mathiasunberath.github.io/talk/emerson_collective_2019/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/emerson_collective_2019/</guid>
      <description>&lt;p&gt;Congratulations to my Co-PIs &lt;a href=&#34;https://www.hopkinsmedicine.org/profiles/results/directory/profile/10003528/tin-liu&#34; target=&#34;_blank&#34;&gt;Tin Yan Alvin Liu&lt;/a&gt; and &lt;a href=&#34;https://www.hopkinsmedicine.org/profiles/results/directory/profile/10004209/zelia-correa&#34; target=&#34;_blank&#34;&gt;Zelia Correa&lt;/a&gt; on being awarded an &lt;a href=&#34;http://www.emersoncollective.com/&#34; target=&#34;_blank&#34;&gt;Emerson Collective Cancer Research Grant&lt;/a&gt;. Our proposal targets digital pathology image analysis, where we will use deep learning for cancer prognostication in uveal melanoma.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excited to be teaching at the First Medical Augmented Reality Summer School</title>
      <link>https://mathiasunberath.github.io/talk/medicalarsummerschool_2019/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/medicalarsummerschool_2019/</guid>
      <description>&lt;p&gt;I am very much looking forward to present our recent work on monocular depth estimation and 3D reconstruction in sinus endoscopy at the &lt;a href=&#34;https://sites.google.com/view/keepmarching/ssia-workshop&#34; target=&#34;_blank&#34;&gt;Inaugural Medical AI Research Collaboration Hub (MARCH) Workshop on Statistical and Shape-based Image Analysis With Applications in Medicine&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excited to give a keynote at the CVPR Workshop on 3D Computer Vision in Medical Environments</title>
      <link>https://mathiasunberath.github.io/talk/cvpr_cvme/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/cvpr_cvme/</guid>
      <description>&lt;p&gt;I have been invited to talk about self-supervised depth estimation and reconstruction in endoscopy at the &lt;a href=&#34;https://cvme.github.io/&#34; target=&#34;_blank&#34;&gt;CVPR Workshop on 3D Computer Vision in Medical Environments&lt;/a&gt;, organized by Vivek Singh, Yao-Jen Chang, and Ankur Kapoor, all with&lt;a href=&#34;https://www.siemens-healthineers.com/en-us/&#34; target=&#34;_blank&#34;&gt;Siemens Healthineers&lt;/a&gt;. It was a great honor joining an outstanding lineup of speakers, including Nicolas Padoy (University of Strasbourg), Zicheng Liu (Microsoft Research), and Serena Yeung (Stanford University).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excited to speak at Utah&#39;s Imaging Elevated Symposium</title>
      <link>https://mathiasunberath.github.io/talk/imaging_elevated/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/imaging_elevated/</guid>
      <description>&lt;p&gt;I am very much looking forward to present our recent work on Augmented Reality and Machine Learning for image-guided procedures at this year&amp;rsquo;s &lt;a href=&#34;https://medicine.utah.edu/imaging-elevated/&#34; target=&#34;_blank&#34;&gt;Imaging Elevated Symposium of Emerging Investigators&lt;/a&gt; organized by the Department of Radiology and Imaging Sciences at the University of Utah. Many thanks to &lt;a href=&#34;https://medicine.utah.edu/faculty/mddetail.php?facultyID=u0114767&#34; target=&#34;_blank&#34;&gt;Frederic Noo&lt;/a&gt; for nominating me!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I will be speaking at the Inaugural MARCH Workshop on Statistical and Shape-based Image Analysis With Applications in Medicine</title>
      <link>https://mathiasunberath.github.io/talk/march_aisinai/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/march_aisinai/</guid>
      <description>&lt;p&gt;I am very much looking forward to present our recent work on monocular depth estimation and 3D reconstruction in sinus endoscopy at the &lt;a href=&#34;https://sites.google.com/view/keepmarching/ssia-workshop&#34; target=&#34;_blank&#34;&gt;Inaugural Medical AI Research Collaboration Hub (MARCH) Workshop on Statistical and Shape-based Image Analysis With Applications in Medicine&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IJCARS Best Paper Awards at MICCAI 2019</title>
      <link>https://mathiasunberath.github.io/talk/2019-11-ijcars_awards/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/2019-11-ijcars_awards/</guid>
      <description>&lt;p&gt;What a finish!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IJCARS Best Paper Award&lt;/strong&gt; for:
Bier, B., Goldmann, F., Zaech, J.N., Fotouhi, J., Hegeman, R., Grupp, R., Armand, M., Osgood, G., Navab, N., Maier, A. and Unberath, M., 2019. Learning to detect anatomical landmarks of the pelvis in X-rays from arbitrary views. International journal of computer assisted radiology and surgery, pp.1-11.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IJCARS Best Paper Award, Runner up&lt;/strong&gt; for:
Fotouhi&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J., Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M., Song, T., Hajek, J., Lee, S.C., Bier, B., Maier, A., Osgood, G., Armand, M. and Navab, N., 2019. Co-localized augmented human and X-ray observers in collaborative surgical ecosystem. International journal of computer assisted radiology and surgery, 14(9), pp.1553-1563. &lt;sup&gt;&amp;#42;&lt;/sup&gt; Joint first authors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Malone Center Seed Grant for research on glaucoma prognostication</title>
      <link>https://mathiasunberath.github.io/talk/2019-10-maloneseedgrant/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/2019-10-maloneseedgrant/</guid>
      <description>&lt;p&gt;Congratulations to my Co-PI &lt;a href=&#34;https://www.hopkinsmedicine.org/profiles/results/directory/profile/10004332/jithin-yohannan&#34; target=&#34;_blank&#34;&gt;Jithin Yohannan&lt;/a&gt; on being awarded a &lt;a href=&#34;https://malonecenter.jhu.edu/2019/10/09/announcing-2019-malone-seed-grant-awards/&#34; target=&#34;_blank&#34;&gt;Malone Center Seed Grant&lt;/a&gt;. Our proposal targets glaucoma prognostication, where we will analyze functional, structural and clinical records to estimate risk of rapid worsening. Glaucoma is the second leading cause of blindness globally, with approximately 79.6 million people expected to be affected by the disease by 2020.  A small subset of glaucoma patients will experience rapid glaucoma worsening that, if untreated, can quickly lead to complete vision loss. Identifying these rapid progressors early is critical for ophthalmologists to perform interventions that can preserve a patient’s eyesight.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presenting task-aware computer assistance on July 19th during the SAOT Innovation Day in Erlangen.</title>
      <link>https://mathiasunberath.github.io/talk/saot_innovation_day/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/saot_innovation_day/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Promotion to Assistant Research Professor</title>
      <link>https://mathiasunberath.github.io/talk/promotion/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/promotion/</guid>
      <description>&lt;p&gt;I am very happy to announce my recent promotion to &lt;strong&gt;Assistant Research Professor&lt;/strong&gt; in the delighted &lt;a href=&#34;https://www.cs.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Deparment of Computer Science&lt;/a&gt; at &lt;a href=&#34;https://www.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Johns Hopkins University&lt;/a&gt;. I am affiliated with the &lt;a href=&#34;https://lcsr.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Laboratory for Computational Sensing and Robotics&lt;/a&gt; and the &lt;a href=&#34;https://malonecenter.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Malone Center for Engineering in Healthcare&lt;/a&gt; and will continue my research on novel techniques for x-ray imaging, machine learning for the interpretation of medical images, and augmented reality for surgical guidance and medical education. I am particularly grateful to my current and previous mentors, including Russell Taylor, Mehran Armand, Nassir Nabab, and Andreas Maier for their exceptional support.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Several Awards at MICCAI 2018 including Young Researcher Award</title>
      <link>https://mathiasunberath.github.io/talk/miccai_awards/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_awards/</guid>
      <description>

&lt;h2 id=&#34;here-s-the-full-list-of-our-awarded-papers&#34;&gt;Here&amp;rsquo;s the full list of our awarded papers:&lt;/h2&gt;

&lt;p&gt;Reckognized with the &lt;strong&gt;MICCAI Young Researcher Award&lt;/strong&gt; and a &lt;strong&gt;Student Travel Award&lt;/strong&gt; for Bastian.&lt;br /&gt;
B. Bier&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech, J. Fotouhi, M. Armand, G. Osgood, N. Navab, A. Maier. “X-ray-transform Invariant Anatomical Landmark Detection for Pelvic Trauma Surgery”&lt;br /&gt;
&lt;sup&gt;&amp;#42;&lt;/sup&gt; These authors are considered joint first authors.&lt;/p&gt;

&lt;p&gt;Awarded with the &lt;strong&gt;Best Paper Award&lt;/strong&gt; in the MICCAI Workshop on Computer Assisted and Robotic Endoscopy. Xingtong also won &lt;strong&gt;Best Presentation&lt;/strong&gt; for his delivery of the work!&lt;br /&gt;
X. Liu, A. Sinha, M. Unberath, M. Ishii, G. Hager, A. Reiter, R. Taylor. &amp;ldquo;Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Awarded with the &lt;strong&gt;Second Best Paper Award&lt;/strong&gt; in the MICCAI Workshop on Computer Assisted and Robotic Endoscopy.&lt;br /&gt;
C. Gao, X.Liu, M. Peven, M. Unberath, A. Reiter. &amp;ldquo;Learning to See Forces: Surgical Force Prediction with RGB-Point Cloud Temporal Convolutional Networks&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two DELTA Grants Awarded</title>
      <link>https://mathiasunberath.github.io/talk/delta_awards/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/delta_awards/</guid>
      <description>

&lt;h4 id=&#34;augmented-reality-for-immersive-3-d-education-in-complex-pelvic-trauma-surgery&#34;&gt;Augmented Reality for Immersive 3-D Education in Complex Pelvic Trauma Surgery&lt;/h4&gt;

&lt;p&gt;The key challenge in treating unstable pelvic fractures, the proposal says, is that the doctors have to do mental mapping of plate positioning and screw trajectories to the fractured anatomy, a skill that surgeons-in-training cannot easily acquire from conventional two-dimensional materials. The grant will be used to create a three-dimensional learning environment that incorporates augmented reality.&lt;/p&gt;

&lt;h4 id=&#34;personalized-augmented-reality-as-an-interactive-teaching-tool-for-facial-anatomy&#34;&gt;Personalized Augmented Reality as an Interactive Teaching Tool for Facial Anatomy&lt;/h4&gt;

&lt;p&gt;Anatomy is one of the most challenging subject matters to both teach and learn, and facial anatomy is one of the most complex regions. This project proposes to design an augmented reality simulator that will help teach this especially challenging aspect of anatomy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Magic Mirror for Facial Anatomy Tracking</title>
      <link>https://mathiasunberath.github.io/project/delta_derma/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/delta_derma/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Significance&lt;/strong&gt;: Anatomy is one of the most challenging subject matters to both teach and learn, and facial anatomy is one of the most complex anatomical regions. Current education relies heavily on cadaveric dissection, supplemented by textbooks; however, human cadavers are expensive and limited in availability, and reading textbooks can feel like a chore to some students. Augmented reality (AR) has the potential to transform the process of learning human anatomy. By blending real and virtual objects, AR allows learners to immerse themselves in the subject matter and take an active role in the learning process. With this technology, students will interact with physiologically accurate and realistic 3-dimensional (3-D) anatomical structures to enhance their understanding of function, form, and spatial relationships. In addition, the 3-D environments are perceived as more lifelike, more interesting, and more enjoyable, which can increase desire to learn and translate into increased knowledge retention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Innovation and Approach&lt;/strong&gt;: We propose the design of an augmented reality simulator for facial anatomy. To create this device, we will first digitize a highly detailed atlas of the human facial cardiovascular and nervous system. Second, we will develop an operating system-independent computer application that uses the webcam video stream to track the users face in real-time and superimpose the facial anatomy map on the user’s own face, mimicking a real-world physical mirror. This simulation device can be integrated into multiple courses currently offered at Johns Hopkins University, including the undergraduate, medical, and nursing schools. We will set up a project homepage with instructions on how to obtain, install, and run the AR learning software, and download links for our system. This tool will allow students to navigate within and around 3-D facial anatomical structures in a completely controlled environment. We hypothesize that students who use our AR environment will find learning more enjoyable and engaging, resulting in higher motivation to study with potential improved learning outcomes. In addition, this “magic mirror” effect (ie. interaction with anatomical structures personalized to the user’s own face) will evoke an emotional response that may have an additional influence on the learning process, for example, a deeper understanding and appreciation of function and structure.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
