<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mathias Unberath on Mathias Unberath</title>
    <link>https://mathiasunberath.github.io/</link>
    <description>Recent content in Mathias Unberath on Mathias Unberath</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>5 Papers at MICCAI 2018</title>
      <link>https://mathiasunberath.github.io/talk/miccai_papers/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_papers/</guid>
      <description>&lt;p&gt;All our 5 manuscripts submitted to MICCAI 2018 were accepted for presentation (3 early accepts)!&lt;/p&gt;

&lt;p&gt;Pre-prints of our papers are available from arXiv and videos are available from &lt;a href=&#34;https://camp.lcsr.jhu.edu/miccai-2018-demonstration-videos/&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt; (&lt;sup&gt;&amp;#42;&lt;/sup&gt; starred authors are considered joint first):&lt;/p&gt;

&lt;p&gt;M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech&lt;sup&gt;&amp;#42;&lt;/sup&gt;, S. C. Lee, B. Bier, J. Fotouhi, M. Armand,
N. Navab. “DeepDRR: A Catalyst for Machine Learning in Fluoroscopy-guided Procedures”.&lt;/p&gt;

&lt;p&gt;J. Fotouhi&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, G. Taylor&lt;sup&gt;&amp;#42;&lt;/sup&gt;, A. G. Farashahi, B. Bier, R. Taylor,
G. Osgood, M. Armand, N. Navab. “Exploiting Partial Structural Symmetry for Patient-Specific Image Augmentation in Trauma Interventions.&lt;/p&gt;

&lt;p&gt;B. Bier&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech, J. Fotouhi, M. Armand, G. Osgood,
N. Navab, A. Maier. “X-ray-transform Invariant Anatomical Landmark Detection for Pelvic Trauma Surgery”.&lt;/p&gt;

&lt;p&gt;A. Preuhs, A. Maier, M. Manhart, J. Fotouhi, N. Navab, M. Unberath.
“Double Your Views – Expoiting Symmetry in Transmission Imaging”.&lt;/p&gt;

&lt;p&gt;J. Hajek&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J. Fotouhi&lt;sup&gt;&amp;#42;&lt;/sup&gt;, B. Bier, S. C. Lee, G. Osgood, A. Maier, M. Armand, N. Navab. “Closing the Calibration Loop: An Inside-out Tracking Paradigm for Augmented Reality in Orthopedic Surgery”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excited to speak at Utah&#39;s Imaging Elevated Symposium</title>
      <link>https://mathiasunberath.github.io/talk/imaging_elevated/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/imaging_elevated/</guid>
      <description>&lt;p&gt;I am very much looking forward to present our recent work on Augmented Reality and Machine Learning for image-guided procedures at this year&amp;rsquo;s &lt;a href=&#34;https://medicine.utah.edu/imaging-elevated/&#34; target=&#34;_blank&#34;&gt;Imaging Elevated Symposium of Emerging Investigators&lt;/a&gt; organized by the Department of Radiology and Imaging Sciences at the University of Utah. Many thanks to &lt;a href=&#34;https://medicine.utah.edu/faculty/mddetail.php?facultyID=u0114767&#34; target=&#34;_blank&#34;&gt;Frederic Noo&lt;/a&gt; for nominating me!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presenting task-aware computer assistance on July 19th during the SAOT Innovation Day in Erlangen.</title>
      <link>https://mathiasunberath.github.io/talk/saot_innovation_day/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/saot_innovation_day/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Several Awards at MICCAI 2018 including Young Researcher Award</title>
      <link>https://mathiasunberath.github.io/talk/miccai_awards/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_awards/</guid>
      <description>

&lt;h2 id=&#34;here-s-the-full-list-of-our-awarded-papers&#34;&gt;Here&amp;rsquo;s the full list of our awarded papers:&lt;/h2&gt;

&lt;p&gt;Reckognized with the &lt;strong&gt;MICCAI Young Researcher Award&lt;/strong&gt; and a &lt;strong&gt;Student Travel Award&lt;/strong&gt; for Bastian.&lt;br /&gt;
B. Bier&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech, J. Fotouhi, M. Armand, G. Osgood, N. Navab, A. Maier. “X-ray-transform Invariant Anatomical Landmark Detection for Pelvic Trauma Surgery”&lt;br /&gt;
&lt;sup&gt;&amp;#42;&lt;/sup&gt; These authors are considered joint first authors.&lt;/p&gt;

&lt;p&gt;Awarded with the &lt;strong&gt;Best Paper Award&lt;/strong&gt; in the MICCAI Workshop on Computer Assisted and Robotic Endoscopy. Xingtong also won &lt;strong&gt;Best Presentation&lt;/strong&gt; for his delivery of the work!&lt;br /&gt;
X. Liu, A. Sinha, M. Unberath, M. Ishii, G. Hager, A. Reiter, R. Taylor. &amp;ldquo;Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Awarded with the &lt;strong&gt;Second Best Paper Award&lt;/strong&gt; in the MICCAI Workshop on Computer Assisted and Robotic Endoscopy.&lt;br /&gt;
C. Gao, X.Liu, M. Peven, M. Unberath, A. Reiter. &amp;ldquo;Learning to See Forces: Surgical Force Prediction with RGB-Point Cloud Temporal Convolutional Networks&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two DELTA Grants Awarded</title>
      <link>https://mathiasunberath.github.io/talk/delta_awards/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/delta_awards/</guid>
      <description>

&lt;h4 id=&#34;augmented-reality-for-immersive-3-d-education-in-complex-pelvic-trauma-surgery&#34;&gt;Augmented Reality for Immersive 3-D Education in Complex Pelvic Trauma Surgery&lt;/h4&gt;

&lt;p&gt;The key challenge in treating unstable pelvic fractures, the proposal says, is that the doctors have to do mental mapping of plate positioning and screw trajectories to the fractured anatomy, a skill that surgeons-in-training cannot easily acquire from conventional two-dimensional materials. The grant will be used to create a three-dimensional learning environment that incorporates augmented reality.&lt;/p&gt;

&lt;h4 id=&#34;personalized-augmented-reality-as-an-interactive-teaching-tool-for-facial-anatomy&#34;&gt;Personalized Augmented Reality as an Interactive Teaching Tool for Facial Anatomy&lt;/h4&gt;

&lt;p&gt;Anatomy is one of the most challenging subject matters to both teach and learn, and facial anatomy is one of the most complex regions. This project proposes to design an augmented reality simulator that will help teach this especially challenging aspect of anatomy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Magic Mirror for Facial Anatomy Tracking</title>
      <link>https://mathiasunberath.github.io/project/delta_derma/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/delta_derma/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Significance&lt;/strong&gt;: Anatomy is one of the most challenging subject matters to both teach and learn, and facial anatomy is one of the most complex anatomical regions. Current education relies heavily on cadaveric dissection, supplemented by textbooks; however, human cadavers are expensive and limited in availability, and reading textbooks can feel like a chore to some students. Augmented reality (AR) has the potential to transform the process of learning human anatomy. By blending real and virtual objects, AR allows learners to immerse themselves in the subject matter and take an active role in the learning process. With this technology, students will interact with physiologically accurate and realistic 3-dimensional (3-D) anatomical structures to enhance their understanding of function, form, and spatial relationships. In addition, the 3-D environments are perceived as more lifelike, more interesting, and more enjoyable, which can increase desire to learn and translate into increased knowledge retention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Innovation and Approach&lt;/strong&gt;: We propose the design of an augmented reality simulator for facial anatomy. To create this device, we will first digitize a highly detailed atlas of the human facial cardiovascular and nervous system. Second, we will develop an operating system-independent computer application that uses the webcam video stream to track the users face in real-time and superimpose the facial anatomy map on the user’s own face, mimicking a real-world physical mirror. This simulation device can be integrated into multiple courses currently offered at Johns Hopkins University, including the undergraduate, medical, and nursing schools. We will set up a project homepage with instructions on how to obtain, install, and run the AR learning software, and download links for our system. This tool will allow students to navigate within and around 3-D facial anatomical structures in a completely controlled environment. We hypothesize that students who use our AR environment will find learning more enjoyable and engaging, resulting in higher motivation to study with potential improved learning outcomes. In addition, this “magic mirror” effect (ie. interaction with anatomical structures personalized to the user’s own face) will evoke an emotional response that may have an additional influence on the learning process, for example, a deeper understanding and appreciation of function and structure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Augmented Reality for Surgery</title>
      <link>https://mathiasunberath.github.io/project/augmented_reality/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/augmented_reality/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Virtual monitors&lt;/strong&gt;: During traditional image-guided interventions, images are displayed on designated monitors that cannot be positioned optimally due to sterility and spatial constraints. With the aid of optical see-through head-mounted display, we present a novel &lt;em&gt;virtual&lt;/em&gt; monitor system that displays medical images in real-time in a mixed reality visualization, which can potentially alleviates the occlusion problem, improves hand-eye coordination, and reduces neck fatigue. The &lt;em&gt;virtual&lt;/em&gt; monitor system consists of a medical imaging modality, a frame grabber, an image processing framework, a data transfer network, and a head-mounted display. We identify the clinical requirements of such system for the targeted clinical usecase and consider medical image properties, visualization mode, perceptual quality, and surgery duration to materialize &lt;em&gt;virtual&lt;/em&gt; monitor systems appropriate for the respective clinical task.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Unprepared environments&lt;/strong&gt;: We present an on-the-fly surgical support system that provides guidance using Augmented Reality and can be used in quasi-unprepared operating rooms. The proposed system builds upon a multi-modality marker and simultaneous localization and mapping technique to co-calibrate an optical see-through head mounted display to a C-arm fluoroscopy system. Then, annotations on the 2-D X-ray images can be rendered as virtual objects in 3-D providing surgical guidance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dynamic calibration&lt;/strong&gt;: In percutaneous orthopedic interventions the surgeon attempts to reduce and fixate fractures in bony structures. The complexity of these interventions arises when the surgeon performs the challenging task of navigating surgical tools percutaneously only under the guidance of 2D interventional X-ray imaging. Moreover, the intra-operatively acquired data is only visualized indirectly on external displays. In this work, we propose a flexible Augmented Reality paradigm using optical see-through head mounted displays. The key technical contribution of this work includes the marker-less and dynamic tracking concept which closes the calibration loop between patient, C-arm and the surgeon. This calibration is enabled using Simultaneous Localization and Mapping of the environment, i.e. the operating theater. In return, the proposed solution provides
&lt;em&gt;in situ&lt;/em&gt; visualization of pre- and intra-operative 3D medical data directly at the surgical site.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Educating Surgeons in Complex Pelvic Trauma</title>
      <link>https://mathiasunberath.github.io/project/delta_ortho/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/delta_ortho/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Significance&lt;/strong&gt;: Treatment of unstable pelvic fractures relies on open reduction followed by internal fixation using surgical plates and screws, a surgery only performed by highly experienced and well-trained surgeons due to the complexity of the pelvic anatomy. The key challenge is the mental mapping of plate positioning and screw trajectories to the fractured anatomy in 3D, a skill that surgeons in training cannot easily acquire from conventional 2D training material such as textbooks, websites, or illustrations from their instructor. This suggests that an inherently 3-dimensional learning environment based on Augmented Reality (AR) has great potential to benefit surgeon training. Innovation and&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Our goal is to develop a collaborative and interactive AR learning platform targeted at improving the understanding of spatial relations of plates, screws, and anatomy during open reduction and internal fixation of pelvic fractures. Our system will be realized as an immersive 3D AR environment based on Microsoft’s HoloLens that visualizes virtual models of anatomy, plates, screws, and simulates the X-ray camera view to the student and instructor. It will allow for manipulation of every component to enable interactive investigation of optimal positioning of implants. This tool is designed to benefit anatomical education, as it will allow students to navigate around 3D structures promoting improved geometric understanding, increased knowledge retention, and a higher appreciation of function and structure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Fluoroscopy-guided Procedures</title>
      <link>https://mathiasunberath.github.io/project/learning_for_fluoro/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/learning_for_fluoro/</guid>
      <description>&lt;p&gt;Machine learning-based approaches outperform competing methods in most disciplines relevant to diagnostic radiology. Interventional radiology, however, has not yet benefited substantially from the advent of deep learning, in particular because of two reasons: 1) Most images acquired during the procedure are never archived and are thus not available for learning, and 2) even if they were available, annotations would be a severe challenge due to the vast amounts of data. When considering fluoroscopy-guided procedures, an interesting alternative to true interventional fluoroscopy is &lt;em&gt;in silico&lt;/em&gt; simulation of the procedure from 3D diagnostic CT. In this case, labeling is comparably easy and potentially readily available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DeepDRR&lt;/strong&gt; on Github&lt;/p&gt;

&lt;iframe style=&#34;display: inline-block;&#34; src=&#34;https://ghbtns.com/github-btn.html?user=mathiasunberath&amp;amp;repo=DeepDRR&amp;amp;type=star&amp;amp;count=true&amp;amp;size=large&#34; scrolling=&#34;0&#34; width=&#34;160px&#34; height=&#34;30px&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
&lt;iframe style=&#34;display: inline-block;&#34; src=&#34;https://ghbtns.com/github-btn.html?user=mathiasunberath&amp;amp;repo=DeepDRR&amp;amp;type=fork&amp;amp;count=true&amp;amp;size=large&#34; scrolling=&#34;0&#34; width=&#34;158px&#34; height=&#34;30px&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
  </channel>
</rss>
