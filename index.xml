<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mathias Unberath on Mathias Unberath</title>
    <link>https://mathiasunberath.github.io/</link>
    <description>Recent content in Mathias Unberath on Mathias Unberath</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>3 Papers at MICCAI 2019</title>
      <link>https://mathiasunberath.github.io/talk/miccai_papers_19/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_papers_19/</guid>
      <description>&lt;p&gt;We will be presenting 3 papers at MICCAI 2019!&lt;/p&gt;

&lt;p&gt;J.-N. Zaech, C. Gao, B. Bier, R. Taylor, A. Maier, N. Navab, M. Unberath.
“Learning to Avoid Poor Images: Towards Task-aware C-arm Cone-beam
CT Trajectories”
Received a MICCAI Graduate Student Travel Award based on its review scores.
Selected for &lt;strong&gt;oral&lt;/strong&gt; presentation.&lt;/p&gt;

&lt;p&gt;L. Fink, S.C. Lee, J.Y. Wu, X. Liu, T. Song, Y. Stoyanova, M. Stamminger,
N. Navab, M. Unberath. &amp;ldquo;LumiPath - Towards Real-time Physicallybased
Rendering on Embedded Devices&amp;rdquo; &lt;a href=&#34;https://github.com/lorafib/LumiPath&#34; target=&#34;_blank&#34;&gt;Link to GitHub&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1903.03837&#34; target=&#34;_blank&#34;&gt;Link to preprint&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;J. Esteban, M. Grimm, M. Unberath, G. Zahnd, N. Navab. &amp;ldquo;Towards fully automatic X-ray to CT registration&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 Papers at MICCAI 2018</title>
      <link>https://mathiasunberath.github.io/talk/miccai_papers/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_papers/</guid>
      <description>&lt;p&gt;All our 5 manuscripts submitted to MICCAI 2018 were accepted for presentation (3 early accepts)!&lt;/p&gt;

&lt;p&gt;Pre-prints of our papers are available from arXiv and videos are available from &lt;a href=&#34;https://camp.lcsr.jhu.edu/miccai-2018-demonstration-videos/&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt; (&lt;sup&gt;&amp;#42;&lt;/sup&gt; starred authors are considered joint first):&lt;/p&gt;

&lt;p&gt;M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech&lt;sup&gt;&amp;#42;&lt;/sup&gt;, S. C. Lee, B. Bier, J. Fotouhi, M. Armand,
N. Navab. “DeepDRR: A Catalyst for Machine Learning in Fluoroscopy-guided Procedures”.&lt;/p&gt;

&lt;p&gt;J. Fotouhi&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, G. Taylor&lt;sup&gt;&amp;#42;&lt;/sup&gt;, A. G. Farashahi, B. Bier, R. Taylor,
G. Osgood, M. Armand, N. Navab. “Exploiting Partial Structural Symmetry for Patient-Specific Image Augmentation in Trauma Interventions.&lt;/p&gt;

&lt;p&gt;B. Bier&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech, J. Fotouhi, M. Armand, G. Osgood,
N. Navab, A. Maier. “X-ray-transform Invariant Anatomical Landmark Detection for Pelvic Trauma Surgery”.&lt;/p&gt;

&lt;p&gt;A. Preuhs, A. Maier, M. Manhart, J. Fotouhi, N. Navab, M. Unberath.
“Double Your Views – Expoiting Symmetry in Transmission Imaging”.&lt;/p&gt;

&lt;p&gt;J. Hajek&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J. Fotouhi&lt;sup&gt;&amp;#42;&lt;/sup&gt;, B. Bier, S. C. Lee, G. Osgood, A. Maier, M. Armand, N. Navab. “Closing the Calibration Loop: An Inside-out Tracking Paradigm for Augmented Reality in Orthopedic Surgery”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Another DELTA Grant Awarded</title>
      <link>https://mathiasunberath.github.io/talk/delta_awards_2019/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/delta_awards_2019/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Significance:&lt;/strong&gt; Machine learning and artificial intelligence (ML &amp;amp; AI) techniques are rapidly becoming a staple in healthcare research, but both healthcare and engineering workforces at Johns Hopkins lag behind in their capacity to advance innovation through interdisciplinary collaboration and to assimilate it into practice. Specifically, healthcare scientists need to understand machine learning algorithms, and engineers need to learn biases in healthcare data that affect performance and utility of algorithms. Addressing these learning needs is critical to foster interdisciplinary collaboration at Johns Hopkins and to enable the School of Medicine to shape the future of healthcare innovations through engineering (Goals 2 and 3 in JHU’s 10 by 20 priorities). Our goal is to develop an online course to equip clinicians and engineers with critical thinking skills to design, analyze, interpret, and report research on ML &amp;amp; AI in healthcare. The primary target population for this course, for purposes of this grant, is clinical care providers, faculties, and graduate trainees (residents, clinical &amp;amp; research post-doctoral fellows, doctoral students, and research staff) in the School of Medicine and the Whiting School of Engineering at Johns Hopkins. While we target a specific population for this grant, we have secured preliminary approval from the Engineering for Professionals Program at JHU for MOOC development of our course, which can enhance global visibility and impact of JHU as a leader in driving healthcare innovation using technology.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Innovation and Approach:&lt;/strong&gt; To our knowledge, none of the courses or curricula at Johns Hopkins address the comprehensive learning needs to build capacity in ML &amp;amp; AI in healthcare. The proposed course integrates principles from engineering and statistical sciences with two complementary parts. The first part is focused upon essential concepts of state-of-the-art machine learning methods. The second part is focused upon core concepts of design, bias, evaluation, and reporting in studies on engineering in healthcare. This course brings together educators with engineering, statistical, epidemiological, and clinical experience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evaluation and Expected Outcome:&lt;/strong&gt; In individual learners, we will evaluate skill acquisition using pre- and post-tests focused on the learning objectives of the course. To quantify impact on the University’s 10 by 20 priorities, we will survey Johns Hopkins personnel completing the course to enumerate downstream effects such as incident journal club sessions they lead within their divisions, new research studies, and collaborations focused upon ML &amp;amp; AI in healthcare.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emerson Collective Cancer Research Fund Awarded</title>
      <link>https://mathiasunberath.github.io/talk/emerson_collective_2019/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/emerson_collective_2019/</guid>
      <description>&lt;p&gt;Congratulations to my Co-PIs &lt;a href=&#34;https://www.hopkinsmedicine.org/profiles/results/directory/profile/10003528/tin-liu&#34; target=&#34;_blank&#34;&gt;Tin Yan Alvin Liu&lt;/a&gt; and &lt;a href=&#34;https://www.hopkinsmedicine.org/profiles/results/directory/profile/10004209/zelia-correa&#34; target=&#34;_blank&#34;&gt;Zelia Correa&lt;/a&gt; on being awarded an &lt;a href=&#34;http://www.emersoncollective.com/&#34; target=&#34;_blank&#34;&gt;Emerson Collective Cancer Research Grant&lt;/a&gt;. Our proposal targets digital pathology image analysis, where we will use deep learning for cancer prognostication in uveal melanoma.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excited to be teaching at the First Medical Augmented Reality Summer School</title>
      <link>https://mathiasunberath.github.io/talk/medicalarsummerschool_2019/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/medicalarsummerschool_2019/</guid>
      <description>&lt;p&gt;I am very much looking forward to present our recent work on monocular depth estimation and 3D reconstruction in sinus endoscopy at the &lt;a href=&#34;https://sites.google.com/view/keepmarching/ssia-workshop&#34; target=&#34;_blank&#34;&gt;Inaugural Medical AI Research Collaboration Hub (MARCH) Workshop on Statistical and Shape-based Image Analysis With Applications in Medicine&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excited to give a keynote at the CVPR Workshop on 3D Computer Vision in Medical Environments</title>
      <link>https://mathiasunberath.github.io/talk/cvpr_cvme/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/cvpr_cvme/</guid>
      <description>&lt;p&gt;I have been invited to talk about self-supervised depth estimation and reconstruction in endoscopy at the &lt;a href=&#34;https://cvme.github.io/&#34; target=&#34;_blank&#34;&gt;CVPR Workshop on 3D Computer Vision in Medical Environments&lt;/a&gt;, organized by Vivek Singh, Yao-Jen Chang, and Ankur Kapoor, all with&lt;a href=&#34;https://www.siemens-healthineers.com/en-us/&#34; target=&#34;_blank&#34;&gt;Siemens Healthineers&lt;/a&gt;. It was a great honor joining an outstanding lineup of speakers, including Nicolas Padoy (University of Strasbourg), Zicheng Liu (Microsoft Research), and Serena Yeung (Stanford University).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excited to speak at Utah&#39;s Imaging Elevated Symposium</title>
      <link>https://mathiasunberath.github.io/talk/imaging_elevated/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/imaging_elevated/</guid>
      <description>&lt;p&gt;I am very much looking forward to present our recent work on Augmented Reality and Machine Learning for image-guided procedures at this year&amp;rsquo;s &lt;a href=&#34;https://medicine.utah.edu/imaging-elevated/&#34; target=&#34;_blank&#34;&gt;Imaging Elevated Symposium of Emerging Investigators&lt;/a&gt; organized by the Department of Radiology and Imaging Sciences at the University of Utah. Many thanks to &lt;a href=&#34;https://medicine.utah.edu/faculty/mddetail.php?facultyID=u0114767&#34; target=&#34;_blank&#34;&gt;Frederic Noo&lt;/a&gt; for nominating me!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I will be speaking at the Inaugural MARCH Workshop on Statistical and Shape-based Image Analysis With Applications in Medicine</title>
      <link>https://mathiasunberath.github.io/talk/march_aisinai/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/march_aisinai/</guid>
      <description>&lt;p&gt;I am very much looking forward to present our recent work on monocular depth estimation and 3D reconstruction in sinus endoscopy at the &lt;a href=&#34;https://sites.google.com/view/keepmarching/ssia-workshop&#34; target=&#34;_blank&#34;&gt;Inaugural Medical AI Research Collaboration Hub (MARCH) Workshop on Statistical and Shape-based Image Analysis With Applications in Medicine&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presenting task-aware computer assistance on July 19th during the SAOT Innovation Day in Erlangen.</title>
      <link>https://mathiasunberath.github.io/talk/saot_innovation_day/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/saot_innovation_day/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Promotion to Assistant Research Professor</title>
      <link>https://mathiasunberath.github.io/talk/promotion/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/promotion/</guid>
      <description>&lt;p&gt;I am very happy to announce my recent promotion to &lt;strong&gt;Assistant Research Professor&lt;/strong&gt; in the delighted &lt;a href=&#34;https://www.cs.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Deparment of Computer Science&lt;/a&gt; at &lt;a href=&#34;https://www.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Johns Hopkins University&lt;/a&gt;. I am affiliated with the &lt;a href=&#34;https://lcsr.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Laboratory for Computational Sensing and Robotics&lt;/a&gt; and the &lt;a href=&#34;https://malonecenter.jhu.edu/&#34; target=&#34;_blank&#34;&gt;Malone Center for Engineering in Healthcare&lt;/a&gt; and will continue my research on novel techniques for x-ray imaging, machine learning for the interpretation of medical images, and augmented reality for surgical guidance and medical education. I am particularly grateful to my current and previous mentors, including Russell Taylor, Mehran Armand, Nassir Nabab, and Andreas Maier for their exceptional support.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Several Awards at MICCAI 2018 including Young Researcher Award</title>
      <link>https://mathiasunberath.github.io/talk/miccai_awards/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/miccai_awards/</guid>
      <description>

&lt;h2 id=&#34;here-s-the-full-list-of-our-awarded-papers&#34;&gt;Here&amp;rsquo;s the full list of our awarded papers:&lt;/h2&gt;

&lt;p&gt;Reckognized with the &lt;strong&gt;MICCAI Young Researcher Award&lt;/strong&gt; and a &lt;strong&gt;Student Travel Award&lt;/strong&gt; for Bastian.&lt;br /&gt;
B. Bier&lt;sup&gt;&amp;#42;&lt;/sup&gt;, M. Unberath&lt;sup&gt;&amp;#42;&lt;/sup&gt;, J.-N. Zaech, J. Fotouhi, M. Armand, G. Osgood, N. Navab, A. Maier. “X-ray-transform Invariant Anatomical Landmark Detection for Pelvic Trauma Surgery”&lt;br /&gt;
&lt;sup&gt;&amp;#42;&lt;/sup&gt; These authors are considered joint first authors.&lt;/p&gt;

&lt;p&gt;Awarded with the &lt;strong&gt;Best Paper Award&lt;/strong&gt; in the MICCAI Workshop on Computer Assisted and Robotic Endoscopy. Xingtong also won &lt;strong&gt;Best Presentation&lt;/strong&gt; for his delivery of the work!&lt;br /&gt;
X. Liu, A. Sinha, M. Unberath, M. Ishii, G. Hager, A. Reiter, R. Taylor. &amp;ldquo;Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Awarded with the &lt;strong&gt;Second Best Paper Award&lt;/strong&gt; in the MICCAI Workshop on Computer Assisted and Robotic Endoscopy.&lt;br /&gt;
C. Gao, X.Liu, M. Peven, M. Unberath, A. Reiter. &amp;ldquo;Learning to See Forces: Surgical Force Prediction with RGB-Point Cloud Temporal Convolutional Networks&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two DELTA Grants Awarded</title>
      <link>https://mathiasunberath.github.io/talk/delta_awards/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://mathiasunberath.github.io/talk/delta_awards/</guid>
      <description>

&lt;h4 id=&#34;augmented-reality-for-immersive-3-d-education-in-complex-pelvic-trauma-surgery&#34;&gt;Augmented Reality for Immersive 3-D Education in Complex Pelvic Trauma Surgery&lt;/h4&gt;

&lt;p&gt;The key challenge in treating unstable pelvic fractures, the proposal says, is that the doctors have to do mental mapping of plate positioning and screw trajectories to the fractured anatomy, a skill that surgeons-in-training cannot easily acquire from conventional two-dimensional materials. The grant will be used to create a three-dimensional learning environment that incorporates augmented reality.&lt;/p&gt;

&lt;h4 id=&#34;personalized-augmented-reality-as-an-interactive-teaching-tool-for-facial-anatomy&#34;&gt;Personalized Augmented Reality as an Interactive Teaching Tool for Facial Anatomy&lt;/h4&gt;

&lt;p&gt;Anatomy is one of the most challenging subject matters to both teach and learn, and facial anatomy is one of the most complex regions. This project proposes to design an augmented reality simulator that will help teach this especially challenging aspect of anatomy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Magic Mirror for Facial Anatomy Tracking</title>
      <link>https://mathiasunberath.github.io/project/delta_derma/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/delta_derma/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Significance&lt;/strong&gt;: Anatomy is one of the most challenging subject matters to both teach and learn, and facial anatomy is one of the most complex anatomical regions. Current education relies heavily on cadaveric dissection, supplemented by textbooks; however, human cadavers are expensive and limited in availability, and reading textbooks can feel like a chore to some students. Augmented reality (AR) has the potential to transform the process of learning human anatomy. By blending real and virtual objects, AR allows learners to immerse themselves in the subject matter and take an active role in the learning process. With this technology, students will interact with physiologically accurate and realistic 3-dimensional (3-D) anatomical structures to enhance their understanding of function, form, and spatial relationships. In addition, the 3-D environments are perceived as more lifelike, more interesting, and more enjoyable, which can increase desire to learn and translate into increased knowledge retention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Innovation and Approach&lt;/strong&gt;: We propose the design of an augmented reality simulator for facial anatomy. To create this device, we will first digitize a highly detailed atlas of the human facial cardiovascular and nervous system. Second, we will develop an operating system-independent computer application that uses the webcam video stream to track the users face in real-time and superimpose the facial anatomy map on the user’s own face, mimicking a real-world physical mirror. This simulation device can be integrated into multiple courses currently offered at Johns Hopkins University, including the undergraduate, medical, and nursing schools. We will set up a project homepage with instructions on how to obtain, install, and run the AR learning software, and download links for our system. This tool will allow students to navigate within and around 3-D facial anatomical structures in a completely controlled environment. We hypothesize that students who use our AR environment will find learning more enjoyable and engaging, resulting in higher motivation to study with potential improved learning outcomes. In addition, this “magic mirror” effect (ie. interaction with anatomical structures personalized to the user’s own face) will evoke an emotional response that may have an additional influence on the learning process, for example, a deeper understanding and appreciation of function and structure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Augmented Reality for Surgery</title>
      <link>https://mathiasunberath.github.io/project/augmented_reality/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/augmented_reality/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Virtual monitors&lt;/strong&gt;: During traditional image-guided interventions, images are displayed on designated monitors that cannot be positioned optimally due to sterility and spatial constraints. With the aid of optical see-through head-mounted display, we present a novel &lt;em&gt;virtual&lt;/em&gt; monitor system that displays medical images in real-time in a mixed reality visualization, which can potentially alleviates the occlusion problem, improves hand-eye coordination, and reduces neck fatigue. The &lt;em&gt;virtual&lt;/em&gt; monitor system consists of a medical imaging modality, a frame grabber, an image processing framework, a data transfer network, and a head-mounted display. We identify the clinical requirements of such system for the targeted clinical usecase and consider medical image properties, visualization mode, perceptual quality, and surgery duration to materialize &lt;em&gt;virtual&lt;/em&gt; monitor systems appropriate for the respective clinical task.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Unprepared environments&lt;/strong&gt;: We present an on-the-fly surgical support system that provides guidance using Augmented Reality and can be used in quasi-unprepared operating rooms. The proposed system builds upon a multi-modality marker and simultaneous localization and mapping technique to co-calibrate an optical see-through head mounted display to a C-arm fluoroscopy system. Then, annotations on the 2-D X-ray images can be rendered as virtual objects in 3-D providing surgical guidance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dynamic calibration&lt;/strong&gt;: In percutaneous orthopedic interventions the surgeon attempts to reduce and fixate fractures in bony structures. The complexity of these interventions arises when the surgeon performs the challenging task of navigating surgical tools percutaneously only under the guidance of 2D interventional X-ray imaging. Moreover, the intra-operatively acquired data is only visualized indirectly on external displays. In this work, we propose a flexible Augmented Reality paradigm using optical see-through head mounted displays. The key technical contribution of this work includes the marker-less and dynamic tracking concept which closes the calibration loop between patient, C-arm and the surgeon. This calibration is enabled using Simultaneous Localization and Mapping of the environment, i.e. the operating theater. In return, the proposed solution provides
&lt;em&gt;in situ&lt;/em&gt; visualization of pre- and intra-operative 3D medical data directly at the surgical site.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Educating Surgeons in Complex Pelvic Trauma</title>
      <link>https://mathiasunberath.github.io/project/delta_ortho/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mathiasunberath.github.io/project/delta_ortho/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Significance&lt;/strong&gt;: Treatment of unstable pelvic fractures relies on open reduction followed by internal fixation using surgical plates and screws, a surgery only performed by highly experienced and well-trained surgeons due to the complexity of the pelvic anatomy. The key challenge is the mental mapping of plate positioning and screw trajectories to the fractured anatomy in 3D, a skill that surgeons in training cannot easily acquire from conventional 2D training material such as textbooks, websites, or illustrations from their instructor. This suggests that an inherently 3-dimensional learning environment based on Augmented Reality (AR) has great potential to benefit surgeon training. Innovation and&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Our goal is to develop a collaborative and interactive AR learning platform targeted at improving the understanding of spatial relations of plates, screws, and anatomy during open reduction and internal fixation of pelvic fractures. Our system will be realized as an immersive 3D AR environment based on Microsoft’s HoloLens that visualizes virtual models of anatomy, plates, screws, and simulates the X-ray camera view to the student and instructor. It will allow for manipulation of every component to enable interactive investigation of optimal positioning of implants. This tool is designed to benefit anatomical education, as it will allow students to navigate around 3D structures promoting improved geometric understanding, increased knowledge retention, and a higher appreciation of function and structure.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
